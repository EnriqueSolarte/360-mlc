# 360-MLC: Multi-view Layout Consistency for Self-training and Hyper-parameter Tuning

<!-- ![](https://enriquesolarte.github.io/360-mlc/img/teaser.svg) -->
![](https://user-images.githubusercontent.com/67839539/205412513-39495ba4-2bf6-47d6-90c8-e948fb22576a.png)

This is the official implementation of 360-MLC, where we propose a novel approach to fine-tune and evaluate pre-trained layout estimation models on new datasets with domain shifts, and, more importantly, no ground truth is required.

For more detailed information, please refer to:
> [**360-MLC: Multi-view Layout Consistency for Self-training and Hyper-parameter Tuning**](https://arxiv.org/abs/2210.12935)          
> Bolivar Solarte, Chin-Hsuan Wu, Yueh-Cheng Liu, Yi-Hsuan Tsai, Min Sun       
> NeurIPS 2022            
> [**[Paper]**](https://arxiv.org/abs/2210.12935), [**[Project Page]**](https://enriquesolarte.github.io/360-mlc/), [**[Video]**](https://youtu.be/x4Vt32egsdU) 

## Video
[![](https://user-images.githubusercontent.com/67839539/205503534-5ea1152e-c855-4b1a-90a0-277bb2731815.png)](https://youtu.be/x4Vt32egsdU)

## Citation
> 
    @inproceedings{solarte2022mlc,
        author={Solarte, Bolivar and Wu, Chin-Hsuan and Liu, Yueh-Cheng and Tsai, Yi-Hsuan and Sun, Min},
        title={360-MLC: Multi-view Layout Consistency for Self-training and Hyper-parameter Tuning},
        booktitle={Advances in Neural Information Processing Systems},
        year={2022},
    }